---
output: pdf_document
---

\chapter{Apache Spark}

# What is Apache Spark

Apache Spark is a computing platform whose goal is make analysis of big data sets fast. Spark extends the MapReduce paradigm to support more computation types. A key feature of Spark is that it can run computation both in-memory, but is also efficient for running complex applications on disk.

The Spark project contains multiple tightly integrated components. Since the system has been designed with the close coupling, this means that improvements in one part of the engine, these improvements are automatically used by other componets. Another benefit of the tight coupling, is that there is a single system to maintain. This can be crucial for large organisations.

The Spark stack contains a number of components.

  * Spark Core: The Core contains the basic functionality of Spark, such as, memory management, fault recovery, and interacting with storage systems. Spark Core provides APIs that enable the other components to access these collections.
  * Spark SQL: This provides an SQL interface for interacting with 
  * Spark Streaming:
  * MLlib:
  * GraphX:
  * Cluster Managers:


# A first Spark instance

There are a number of prelimary steps we need to take before beginning an analysis. First, we set the environment variable `SPARK_HOME`. This can be in done in our `bashrc` file, or in R itself, via
```{r eval=FALSE, echo=1}
Sys.setenv(SPARK_HOME="/path/to/spark/")
Sys.setenv(SPARK_HOME="/data/ncsg3/spark-1.4.1-bin-hadoop2.6/")
```

Then we load the `SparkR` package
```{r eval=FALSE}
library("SparkR")
```

\noindent Next we initialise the Spark cluster and create a `SparkContext` 
```{r eval=FALSE}
sc = sparkR.init(master="local")
```
\noindent The `sparkR.init` function has number of arguments. In particular, if we want to use any Spark packages, these should be specified during the initialisation stage. So if we wanted to load in a csv file, we would need something like
```{r eval=FALSE}
sc = sparkR.init(sparkPackages="com.databricks:spark-csv_2.11:1.0.3")
```

\noindent When we finish our Spark session, we should terminate the Spark context via
```{r eval=FALSE}
sparkR.stop()
```

# Resilient Distributed Datasets (RDD)

The core feature of Spark is the resilient distributed dataset (RDD). An RDD is an abstraction\sidenote{By abstraction we simply mean, we don't worry about how or where the data set is stored.} that helps us deal with big data. An RDD is a distributed collection of elements (including data and functions). In Spark, everything we do revolves around RDDs. Typically, we may want to create,  transform or operate on the distributed data set. Spark automatically handles had the data is distributed across your computer/cluster and parallelises operations where possible.

## Example: Moby Dick

For this example, we are using the Moby Dick text, downloaded from the Project Gutenberg website. Assuming that we are already in a Spark session, we can read in the text using the `textFiles` function
```{r eval=FALSE}
moby = SparkR:::textFile(sc, "data/moby_dick.txt")
```
\noindent There are two keys points to note. First, we pass the Spark instance object `sc` as an argument.\sidenote{In R, `::` is used to access functions that have been exported by a package, i.e. methods that appear in the NAMESPACE file. However, there are some functions in the package that the author may want to remain private, these can be accessed using `:::`.}  Second,we are using `:::` to access an non-exported function from `SparkR`\sidenote{SparkR was only integrated into Spark in June 2015, so the API is still being finalised.}. This is because the API for SparkR has yet to be finalised. So it is likely that some things in this chapter will break in a few months time. The `moby` object is an RDD
```{r eval=FALSE, tidy=FALSE}
R> moby
# MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:-2
```
\noindent Once we have an RDD object, there are two options available: *transformation* and *action*. A transformation operation, constructs a new RDD by using the previou one. For example, suppose we want to extract the lines that contain the word `Moby` in our data set. This is a standard operation, we have a dataset and we want to remove certain values. We create a function called `get_moby` that only returns `TRUE` or `FALSE`. The `filterRDD` function then retains any rows that are `TRUE`, i.e.
```{r eval=FALSE, tidy=FALSE}
get_moby = function(x) 
  "Moby" %in% strsplit(x, split = " ")[[1]]
mobys = SparkR:::filterRDD(moby, get_moby)
```
\noindent Again this is a functional approach to programming and is similar to the `apply` family. 

An action computes a result based on an RDD. The result is either displayed or stored somewhere else on the system. For example, if we want to know how many rows contain the word `Moby`, we use the count function
```{r eval=FALSE}
## The answer is 77 BTW
count(mobys)
```
\noindent Spark deaks with Transformations and actions in two different ways. Similar to `dplyr`, Spark uses lazy evaluation, that is it only performs that computation when it is used by an action. In the example above, the `textFile` and `filterRDD` commands are run only when we use `count`. 

Similar to `dplyr`, lazy evaluation is essential when working with big data. If we consider the example above. If SparkR actually ran `textFile` straight away, this would use up a load of disk space. This is a waste, since we immediately filter out the vast majority of the text. Instead, SparkR (and hence Spark), takes the chain of transformations, and computes just the data needed to get the result.

Spark's RDDs are (by default) recomputed each time you run an action on them.\sidenote{Alternatively, we could use `cache(moby)`, which is the same as `persist` with the default level of storage.} To reuse RDD's in multiple operations we can ask Spark to persit it via
```{r eval=FALSE}
## There are different levels of storage
persist(mobys, "MEMORY_ONLY")
```

\noindent If you are not planning on reusing the object, don't use persist.

To summarise, every Spark seesion will have a similar structure.
\begin{enumerate}
\item Create a resilient distributed dataset (RDD).
\item Transform and manipulate the data set.
\item For key data sets, use `persist` for efficieny.
\item Retrieve the results via an action such as `count`.
\end{enumerate}

# Loading data: creating RDDs

There are two ways of creating an RDD. Either by parallelizing an existing dataset, or from an external data source, such as a database or csv file.

The easiest way to create an RDD file is from an existing data set and pass it to the `parallelize` function. However, this probably means the data is realively small and you don't need to use Spark. Nevertheless, it's when learning Spark/SparkR, since you can quickly test and protype code. For example, to create an RDD representation of the vector `1:100`, we would use

```{r eval=FALSE}
vec_sp = SparkR:::parallelize(sc, 1:100)
```

\noindent Typically, we would want to load data from external data sets. This could, for example, be from a text file using `textFile` described above, or from CSV file (again via `textFile`), provided you have loaded the correct library.



# Example: Spark dataframes

Suppose we have already initialised a Spark context. To use Spark dataframes, we create an SQLContext 
```{r eval=FALSE}
sqlContext = sparkRSQL.init(sc)
```

\noindent The SQLContext allows us to create dataframes from a local R data frame, 

```{r eval=FALSE}
chicks_sp = createDataFrame(sqlContext, chickwts) 
```

\noindent or from other data sources, such as CSV files or a Hive table. If we examine the newly created object, we get

```{r eval=FALSE}
R> chicks_sp
# DataFrame[weight:double, feed:string]
```

\noindent An S3 method for `head` is also available, so

```{r eval=FALSE, tidy=FALSE}
R> head(chicks_sp, 2)
#  weight      feed
#1    179 horsebean
#2    160 horsebean
```

\noindent We can extract columns using the dollar notation, `chicks_sp$weight` or using `select`
```{r eval=FALSE}
R> select(chicks_sp,  "weight")
# DataFrame[weight:double]
```

\noindent We can subset or filter the data frame using the `filter` function
```{r eval=FALSE}
filter(chicks_sp, chicks_sp$feed == "horsebean")
```

\noindent Using Spark data frames, we can also easily group and aggregate data frames (similar to the `dplyr` syntax). For example, to count the number of chicks in each feed group, we group, and then summarise:

```{r eval=FALSE, tidy=FALSE}
chicks_cnt = groupBy(chicks_sp, chicks_sp$feed) %>%
  summarize(count=n(chicks_sp$feed))
```
\noindent Then use `head` to view the top rows

```{r eval=FALSE, tidy=FALSE}              
head(chicks_cnt, 2)
#      feed count
#1   casein    12
#2 meatmeal    11
```
\noindent We can also use arrange the data by the most common group
```{r eval=FALSE, tidy=FALSE}
arrange(chicks_cnt, desc(chicks_cnt$count))
```

## Example: Bootstraping

In this example\sidenote{This example has been adapted from the rstudio blog post at \url{http://goo.gl/pk3Ax9}} we will create a large dataset that we 

These examples are nice, but you can also use the power of Spark for more common data science tasks. Letâ€™s sample a dataset to generate a large RDD, which we will then summarise via bootstrapping. Instead of parallelizing numbers, I will now parallelize dataframe samples.


# Resources

 * Apache Spark homepage\sidenote{\url{https://spark.apache.org/}}
 * Learning Spark [@Karau2015]
 * Advanced analytics with Spark [@Ryza2014]

\clearpage
