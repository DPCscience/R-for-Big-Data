---
title: "R for Big Data"
author: "Colin Gillespie and Robin Lovelace"
output: rmarkdown::tufte_handout
---

# Introduction

## What is 'Big Data'?

## Why R for Big Data?

## Why not R for Big Data?

## Alternatives to R

# Principles of efficient R programming

When dealing with small datasets, speed considerations
are often irrelevant as powerful computers can solve
the problem through 'brute force'. However, when you
start dealing with larger datasets, a slight change in
the *computational efficiency* of the implementation
can make the difference between completing an analysis
overnight and not completing it at all.

*Computational efficiency* is analogous to energy efficiency
and is a programmer's way of saying 'I get this much boom for
my buck'? This is generally measured by *benchmarking*,
testing the amount of
computer time taken to perform a given task using different
implementations. It is enlightening to perform such tests on
a small sample of your data to decide how to write your
final code so the processing completes quickly and without
consuming excessive computer resources such as RAM. The below
example illustrates the process of benchmarking.

```{r}
# TODO: Example of benchmarking here
```

# Loading Big Data 

```{r}
# (e.g. via dplyr and RODBC - maybe something like the new and superfast MonetDB)
```

## Text files

Data stored as text files are files that are human-readable when displayed
in a 'plain text' editor such as Microsoft Notepad, Vim or RStudio.
Plain text files are the basis of computing.^[Most
programs can be represented as large collections of scripts written in
plain text. RStudio,
for example, is written in 100s of lines of plain text files, all of which
can be viewed on-line
(see [github.com/rstudio/rstudio](https://github.com/rstudio/rstudio)).
This tutorial was written as a UTF-8 encoded plain text '.Rmd' file.
] 
The advantages of plain text files are:

- Simplicity: quick and easy to understand their contents
- Compatibility: text files work with most software packages
- Portability: text files are quick and easy to load, save and share

The disadvantages of plain text files for Big Data are that they can become
unwieldy, even when compressed (remember the 5.6 Gb file from the introduction),
and their ease of modification: text files are certainly not a highly secure
data format.

The most common format of text file is the trusty .csv file, in which
each column is separated by a comma.

```{r}
write.csv(x = cars[1:3,]) # write a .csv file to the screen
```

Note that in the result from the above operation text strings such as
`"speed"` are enclosed in quote markes wheras raw numbers are not.

> **Challenge**: Save a .csv file of the full 'cars' dataset and open it with a plain text editor.

## Files from proprietary formats

## Data from an SQL database

## MonetDB

## Comparing load times using different approaches

# Manipulating large datasets with R

Rarely are datasets provided in a form that is analysis-ready.
Usually, datasets must be manipulated, 'wrangled' or
'munged' into a format that is suitable for performing the
analysis that you want to do.

## Data manipulation with **dplyr**

## The **data.table** approach

## Revolution R

```{r}
# TODO: more methods for data manipulation
```

# Analysing Big Data with R

# Making sense of Big Data via visualisation

```{r}
# TODO (RL): Add analogy to refining to reduce volume
```

# Using C++ for 'heavy lifting' 

# Running R 'in the cloud'

