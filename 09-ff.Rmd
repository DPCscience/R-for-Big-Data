---
output: word_document
---


\chapter{The ff package}

#  Introduction

The `ff` package provides access to data stored on your hard desk, i.e. data isn't stored in memory. It allows efficient indexing, retrieval and sorting of vectors. This means that we can now load and analyse much larger data sets\sidenote{These days, a standard hard drive is around 1TB, whereas RAM is around 16BG.}. However, accessing data from your hard drive is slower than from memory. 

Since we are no longer loading data directly into memory, this results in non-standard R code. The `ff` package only provides the building blocks, that is it doesn't offer many statistical functions and offers no support for characters. 

\marginnote{The successor to `ffbase` is currently available on github, \url{https://github.com/edwindj/ffbase2}. The `ffbase2` aims to integrate `ff` with `dplyr`.}
The `ffbase` package extends `ff` by providing `ff` versions of commonly used operations, such as standard mathematical operators  and common functions,
`c()`, `duplicated()` and `which()` (implemented as `ffwhich()`).

\subsubsection*{Exercise: How big is your hard drive?}

\subsection*{Aside: HDD vs SSD}

Unless you have a fairly expensive laptop, you've probably got a standard Hard Disk drive (HDD). HDDs were first introduced by IBM in 1956. Data is stored using magnetism on a rotating platter. The faster the platter spins, the faster the HDD can perform. Many laptop drives spin at either 5400RPM (Revolutions per Minute) or 7200RPM. The major advantage of HDD is that they are cheap; a 1TB laptop is becoming
standard.

\begin{marginfigure}
\centering
\includegraphics[]{figures/Laptop-hard-drive-exposed.jpg}
\caption{A standard 2.5" hard drive, found in most laptops. Figure credit: \url{https://en.wikipedia.org/wiki/Hard_disk_drive}.}
\end{marginfigure}

Solid state drives (SSD), can be thought of large, but more sophisticated versions of your USB stick. They have no moving parts and information is stored in microchips. Since there are no moving parts, reading/writing is much quicker\sidenote{SSDs have other benefits, such as being quieter, faster booting time for the OS, required less power (more battery life).

The read/write speed for standard HDD is usually in the region of 50-120MB/s (usually nearer the 50 than the 120). For SSDs, speeds are typically over 200MB/s and for top of the range models, it is closer to 500MB/s. If you're wondering, read/write speeds for RAM is around 2-20GB/s. So at best, SSDs are at least one order of magnitude slower than RAM.

# Importing data

The `ff` package provides a number of functions to read in data. All the `read.*` base R functions have `ff` equivalents that are used in the same way. For example, after loading the package\sidenote{We can install the package in the usual way `install.packages("ff")`.}

```{r message=FALSE}
library("ff")
```

\noindent To illustrate, we'll use a simple csv file\marginnote{This CSV file is pretty small, and the `ff` package is complete overkill. However, using small data set is handy for illustrating key concepts.} that comes with the `r4bd` package.\marginnote{There are two key classes in the `ff` package. `ff` for vector and `ffdf` for data frames.}

```{r, cache=FALSE}
## `get_rand()` Returns the full path
filename = r4bd::get_rand()
ffx = read.csv.ffdf(file=filename, header = TRUE)
```

\noindent We can use (some) standard R functions to query the data set.

```{r, echo=FALSE}
## Only 10000 rows (small) 
dim(ffx)
```

\noindent However since `ffx` isn't a standard data frame, not all functions work, for example

```{r eval=FALSE}
colSums(ffx) # produces the following error:

## Error in colSums(ffx) : 'x' must be an array ...
```

\noindent The `ETLutils` package provides further methods for importing data from SQL databases, such as SQLite, Oracle, Hive, and MySQL. 

# Data chunks

The key idea with `ffdf` objects, is that we no longer manipulate objects in one go, but instead, we use
chunks of data. Essentially, we split the data set up into smaller pieces that can be manipulated by R, and process them one-by-one. The `ff` package is a much more efficient solution than the naive approach of manually splitting your data into separate files, and using numerous `read.csv` calls.

The `chunk` function creates a sequence of range indexes using a syntax similar to `seq`.\marginnote{You can easily test how nerdy you are, by Googling `chunk image` and see how many references are returned to the Goonies film.} Since this data set is small, we only have a single chunk, i.e.
```{r}
length(chunk(ffx))
```
\noindent To make this section more realistic\marginnote{We usually don't need to bother with `length.out`, but since the data set is small, we only have one chunk.}, we'll manually specify the number of chunks using the `length.out`. The `chunk` function returns a list of ranges
```{r}
chunk(ffx, length.out=10)[[1]]
```

\noindent Since we are now dealing with chunks, this makes standard data analysis a pain. For example, suppose we just want to find the minimum value of the matrix. If `ffx` was a standard data frame, we would just use `min(ffx)`. However, `ffx` isn't a standard R object. Instead, we need to loop over the chunks and keep track of the result, e.g.

```{r}
m = numeric(10)
chunks = chunk(ffx, length.out=10)
for(i in seq_along(chunks))
  m[i] = min(ffx[chunks[[i]],])
min(m)
```

\noindent Since we are dealing with out of memory objects, standard rules about copying objects no longer apply\marginnote{If you've come across reference classes in R, it's the same idea.}. In particular, when we copy objects, we are passing by reference. For example, when we change `ffy`

```{r}
ffy = ffx
ffy[1, 1]  = 0
```

\noindent we have also changed `ffx`

```{r}
ffx[1, 1]
```

\noindent It's a trade off between large objects and side-effects.  

\newthought{At this point} it's worthwhile thinking about speed comparisons with `readr`, via a quick benchmark. First we create a test data set

```{r, echo=FALSE, eval=FALSE}
r4bd::create_rand("example.csv", 1e6)
```

\noindent Then time reading in the files
```{r, echo=FALSE, eval=FALSE}
system.time(ffx <- ff::read.csv.ffdf(file="/tmp/tmp.csv", header = TRUE))
system.time(x <- readr::read_csv("/tmp/tmp.csv"))
```

\noindent On my machine, the `readr` function is an order of magnitude faster. This is what we would expect. The `ffdf` version is also preparing the data for future read/write access from the hard drive. Also, the `readr` variant is limited by your RAM. So if your file is too large, you will get an error

```{r eval=FALSE}
R> x = readr::read_csv("very_big.csv")
# Error: cannot allocate vector of size 12.8 Gb
```


## ff Storage

When data is the `ff` format, processing is faster than using the standard `read.csv`/`write.csv` combination. However, converting data into `ff` format can be time consuming; so keeping data in `ff` format is helpful. When you load in an `ff` object, there is a corresponding file(s) created on your hard disk

```{r} 
filename(ffx)
```

\noindent This make moving data around a bit more complicated. The package provides helper functions, `ffsave` and `ffload`, which zips/unzips `ff` object files. However, the `ff` files are not platform-independent, so some care is needed when changing operating systems.

# The ffbase package

The `ff` package supplies the tools for manipulating large data sets, but provides few statistical functions. Conceptually, chunking algorithms are straightforward. The program reads a chunk of data into memory, performs intermediate calculations, saves the results and reads the next chunk. This process repeats until the entire dataset is processed. Unfortunately, many statistical algorithms have not been written with chunking in mind.

The `ffbase`\sidenote{\url{http://github.com/edwindj/ffbase} package adds basic statistical functions to `ff` and `ffdf` objects. It tries to make the code more R like and smooth away the pain of working with `ff` objects. It also provides an interface with `big*` methods.

`ffbase` provides S3 methods for a number of standard functions `mean`, `min`, `max`, and standard arithmetic operators (see `?ffbase` for a complete list) for `ff` objects. This removes some of the pain when dealing with `ff` objects. 

\newthought{The} `ffbase` package also provide access to other packages that handle large data sets. In particular,

* `biglm`: Regression for data too large to fit in memory;
* `biglars`: Scalable Least-Angle Regression and Lasso.
* `bigrf`: Big Random Forests: Classification and Regression Forests for Large Data Sets.
* `stream`: Infrastructure for Data Stream Mining.

# The Biglm package

Linear models (lm) are one of the most basic statistical models available. The simplest regression model is 
\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]
where $\epsilon_i \sim N(0, \sigma^2)$. This corresponds to fitting a straight line through some points. So $\beta_0$ is the  $y$-intercept and $\beta_1$ is the gradient. In the more general multiple regression model, there are $p$ predictor variables
\begin{equation}
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip} + \epsilon_i, 
\end{equation}
where $x_{ij}$ is the $i^\text{th}$ observation on the $j^\text{th}$ independent variable. Equation (\ref{}) can be written neatly in matrix notation as
\[
\bm{Y} = X \bm{\beta} + \bm{\epsilon}
\]
with dimensions
\[
[n\times 1]= [n\times (p+1)] ~[(p+1)\times 1] + [n \times 1 ]\;,
\]
where
\begin{itemize}
\item $\bm{Y}$ is the response vector - (dimensions $n \times 1$);
\item $X$ is the design matrix - (dimensions $n \times (p+1)$);
\item $\bm{\beta}$ is the parameter vector - (dimensions $(p+1) \times 1$);
\item $\bm{\epsilon}$ is the error vector - (dimensions $n \times 1$).
\end{itemize}
\noindent The goal of regression is to estimate $\bm{\beta}$ with $\bm{\hat\beta}$. It can be shown that 
\begin{equation}
\bm{\hat\beta} = (X^T X)^{-1} X^T \bm{Y} \;.
\end{equation}
\noindent Our estimate of $\bm {\hat \beta}$ will exist provided that $(X^T X)^{-1}$
exists, i.e. no column of $X$ is a linear combination of other columns.

For a least squares regression with a simple size of $n$ training examples and $p$ predictors, it takes:

 * $O(p^2n)$ to multiply $X^T$ by $X$
 * $O(pn)$ to multiply $X^T$ by $\bm{Y}$
 * $O(p^3)$ to compute the LU (or Cholesky) factorization of $X^TX$ that is used to compute the product of $(X^TX)^{-1} (X^T\bm{Y})$.

\noindent Since $n >> p$, this means that the algorithm scales with order $O(p^2 n)$. As well as taking a long time to calculate, the memory required also increases. The R implementation of `lm` requires $O(np + p^2)$ in memory. But this can be reduced by constructing the model matrix in chunks. The `biglm`'s algorithm is based on algorithm AS 274\sidenote{\url{http://lib.stat.cmu.edu/apstat/274}}, @Miller1992. It works by updating the Cholesky decomposition with new observations. So for a model with $p$ variables, only the $p \times p$ (triangular) Cholesky factor and a single row of data needs to be in the memory at any given time. The `biglm` package does not do the chunking for you, but `ffbase` provides a handy S3 wrapper, `bigglm.ffdf`.

For an example of using `biglm`, see the blog post at \url{http://goo.gl/iBPkTp} by Bnosac.









