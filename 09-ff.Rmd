---
output: pdf_document
---

```{r eval=FALSE, echo=FALSE}
library(readr)
library(ffbase)
# Note - this needs to be run once for the book to build
# Set wd if it's buiding in 'notes' folder
if(grepl(pattern = "notes", getwd())){
  library(knitr)
  knitr::opts_knit$set(root.dir = "../")
  
  if(!file.exists("data/rand.csv")){
    # save random data frame
    x = data.frame(x=rnorm(1e7), y=rnorm(1e7))
    # system.time(write.csv(x, file="data/rand.csv", row.names=FALSE))
    system.time(write_csv(x, path = "data/rand.csv")) # slightly faster
    rm(x)
  }
  
}

```

#  ff: classes for representing (large) atomic data

The `ff` package provides access to data stored on your hard desk, i.e. data isn't stored in memory. It allows efficient indexing, retrieval and sorting of vectors. While `ff` is nice, it means that everything is of type `ff`, which results in non-standard R code. The package only provides the building blocks; it doesn't offer many statistical functions and offers no support for characters. 

`ffbase` extends `ff` by providing 'ff versions' of commonly used operations,
including standard mathematical operators as well as
`c()`, `duplicated()` and `which()` (implemented as `ffwhich()`).


## Importing data

The `ff` package provides a number of functions to read in data. All the `read.*` base R functions have `ff` equivalents that are used in the same way. For example, after loading the package

```{r message=FALSE}
library("ff")
```

we can load a data set via

```{r, cache=FALSE}
system.time(ffx <- read.csv.ffdf(file="data/rand.csv", header = TRUE))
```

library(readr)
```{r, echo=FALSE, eval=FALSE}
system.time(x <- read_csv("data/rand.csv"))
system.time(x <- read.csv("data/rand.csv"))
# It seems that read_csv is much faster than either read.csv or read.csv.ffdf
# (3 vs 61 vs 372 (!) seconds on my laptop) - if you can reproduce this i 
# suggest we don't use read.csv.ffdf as it seems dodgy.
```

We can use (some) standard R functions to query the data set.

```{r, echo=FALSE}
dim(ffx)
```

However, since `ffx` isn't a standard data frame, not all functions work, for example

```{r eval=FALSE}
colSums(ffx) # produces the following error:

## Error in colSums(ffx) : 'x' must be an array ...
```


The `ETLutils` package provides further methods for importing data from SQL databases, such as SQLite, Oracle, Hive, and MySQL. 

## Data chunks

This section isn't quite right - `ffx[,1]` is numeric not class `ff`

The `chunk` function creates a sequence of range indexes using a syntax similar to `seq`. For this particular data set, the data is split into `r length(chunk(ffx))` chunks:

```{r}
chunk(ffx)[[1]]
```

Since we are now dealing with chunks, this makes standard data analysis a pain. For example, to find the minimum value of the first column in our data set, we need loop over the chunks

```{r}
m = NULL
for(i in chunk(ffx))
  m = min(ffx[i, 1], m, na.rm=TRUE)
m
```

Since we are dealing with out memory objects, standard rules about copying objects no longer apply. In particular, when we copy objects, we are passing by reference. For example, when we change `ffy`

```{r}
ffy = ffx
ffy[1, 1]  = 0
```

we have also changed `ffx`

```{r}
ffx[1, 1]
```

It's a trade off between large objects and side-effects.  





## ff Storage

When data is `ff` format, processing is fast. However, converting data into `ff` format can be time consuming; so keeping data in `ff` format is helpful. When you load in an `ff` object, there is a corresponding file(s) on your hard disk

```{r}
filename(ffx)
```

This make moving data around a bit more complicated. The package provides helper functions, `ffsave` and `ffload`, which zips/unzips `ff` object files. However, the `ff` files are not platform-independent, so some care is needed when changing operating systems.


## ffbase


The `ff` package supplies the tools for manipulating large data sets, but provides few statistical functions. Conceptually, chunking algorithms are straightforward. The program reads a chunk of data into memory, performs intermediate calculations, saves the results and reads the next chunk. This process repeats until the entire dataset is processed. Unfortunately, many statistical algorithms have not been written with chunking in mind.

The [`ffbase`](http://github.com/edwindj/ffbase) package adds basic statistical functions to `ff` objects. It tries to make the code more R like and smooth away the pain of working with `ff` objects. It also provides an interface with `big*` methods.

`ffbase` provides S3 methods for a number of standard functions `mean`, `min`, `max`, and standard arithmetic operators (see `?ffbase` for a complete list). This removes some of the pain when dealing with `ff` objects. So instead of looping through chunks to find the minimum, we can do

```{r message=FALSE}
library("ffbase")
min(ffx[,1])
```

This is an example of an S3 generic. When we call the `min` function, we get passed to the `min.ff` function.

The `ffbase` package also provide access to other packages that handle large data sets. In particular,

 * `biglm`: Regression for data too large to fit in memory
 * `biglars`: Scalable Least-Angle Regression and Lasso
 * `bigrf`: Big Random Forests: Classification and Regression Forests for Large Data Sets
 * `stream`: Infrastructure for Data Stream Mining
 
### Biglm

Linear models (lm) are one of the most basic statistical models availables. The simplest regression model is 
\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]
where $\epsilon_i \sim N(0, \sigma^2)$. This corresponds to fitting a straight line through some points. So $\beta_0$ is the  $y$-intercept and $\beta_1$ is the gradient. In the more general multiple regression model, there are $p$ predictor variables
\begin{equation}
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip} + epsilon_i, 
\end{equation}
where $x_{ij}$ is the $i^\text{th}$ observation on the $j^\text{th}$ independent variable. Equation (\ref{}) can be written neatly in matrix notation as
\[
\mathbf{Y} = X \mathbf{\beta} + \mathbf{\epsilon}
\]
with dimensions
\[
[n\times 1]= [n\times (p+1)] ~[(p+1)\times 1] + [n \times 1 ]\;,
\]
where
\begin{itemize}
\item $\mathbf{Y}$ is the response vector - (dimensions $n \times 1$);
\item $X$ is the design matrix - (dimensions $n \times (p+1)$);
\item $\mathbf{\beta}$ is the parameter vector - (dimensions $(p+1) \times 1$);
\item $\mathbf{\epsilon}$ is the error vector - (dimensions $n \times 1$).
\end{itemize}
The goal of regression is to estimate $\mathbf{\beta}$ with $\mathbf{\hat\beta}$. It can be shown that 
\begin{equation}
 \mathbf{\hat\beta} = (X^T X)^{-1} X^T \mathbf{Y} \;.
\end{equation}
Our estimate of $\mathbf {\hat \beta}$ will exist provided that $(X^T X)^{-1}$
exists, i.e. no column of $X$ is a linear combination of other columns.

For a least squares regression with a simple size of $n$ training examples and $p$ predictors, it takes:

 * $O(p^2n)$ to multiply $X^T$ by $X$
 * $O(pn)$ to multiply $X^T$ by $\mathbf{Y}$
 * $O(p^3)$ to compute the LU (or Cholesky) factorization of $X^TX$ that is used to compute the product of $(X^TX)^{-1} (X^T\mathbf{Y})$.
 
Since $n >> p$, this means that the algorithm scales with order $O(p^2 n)$. As well as taking a long time to calculate, the memory required also increases. The R implementation of `lm` requires $O(np + p^2)$ in memory. But this can be reduced by constructing the model matrix in chunks. The `biglm`'s algorithm is based on [algorithm AS 274](http://lib.stat.cmu.edu/apstat/274), Miller, A, (1991). It works by updating the Cholesky decomposition with new observations. So for a model with $p$ variables, only the $p \times p$ (triangular) Cholesky factor and a single row of data needs to be in the memory at any given time. The `biglm` package does not do the chunking for you, but `ffbase` provides a handy S3 wrapper, `bigglm.ffdf`.




















 
 

 