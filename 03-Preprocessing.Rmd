---
output: pdf_document
---

\chapter{Preprocessing and Loading Data} {#pre}

R is ideal for handling many data-related tasks but not everything.
As mentioned in the introduction, there are various ways to
preprocess large files outside R to make them easier to handle.
Here we will explore some of the options.

For data stored in large text files we can use
'streaming' utilities before reading it into R. With tools such as
*sed*\sidenote{\url{https://www.gnu.org/software/sed/manual/sed.html}}
(a 'stream editor' included on most Unix-based systems),
split\sidenote{\url{https://en.wikipedia.org/wiki/Split\_\%28Unix\%29}} and
csvkit,\sidenote{\url{https://csvkit.readthedocs.org/en/latest/}} a 10 Gb .csv can be
broken up into smaller chunks before being loaded into R.
Furthermore, these tools will run well on a puny laptop.
Here's an 
example of trying (and failing!) to load a large dataset into R.
We recommend you don't
run this code:^[For
more information on the origin and content of this dataset, see chapter 10.]

```{r, eval=FALSE, tidy=FALSE}
dir.create("data") # create folder for data
url <- "http://download.cms.gov/nppes/NPPES_Data_Dissemination_Aug_2015.zip"

# download a large dataset - don't run
library("downloader") # needs to be installed
download(url, destfile = "data/largefile.zip")
## 550600K .......... ...... 100% 1.26M=6m53s

# unzip the compressed file, measure time
system.time( 
  unzip("data/largefile.zip", exdir = "data")
  )
##    user  system elapsed 
##  34.380  22.428 193.145

bigfile <- "data/npidata_20050523-20150809.csv"
file.info(bigfile) # file info (not all shown)
##       size: 5647444347
```

\noindent The above code illustrates how R can be used to download,
unzip and present information on a giant .csv file, in completely reproducible workflow.
Note that it's 5.6 GB in size and took over 3 minutes to unzip!
The following code requires a 64 bit R installation and will not work on
many laptops.^[Reading
the 5.6 GB file would also fail on many desktops.
The file took over 15 minutes using `read.csv` and half that using `read_csv` from the **readr** package on a Fourth Generation Intel i7 desktop with 64 GB RAM.
Half of this RAM became occupied by R!
]

```{r, eval=FALSE}
system.time(df <- read.csv(bigfile))
## Error (from 32 bit machine): cannot allocate vector of 32.0 Mb
```

\noindent There are ways to better handle such large datasets such as using faster read-in functions such as `read_csv()` from the **readr** package. For now, just remember that reading large datasets into R can be tricky and time-consuming. Preprocessing outside R, as illustrated below, can help.

# Splitting files with Unix tools

The Unix utility **split** can be used to split large files, like the one we tried to load above, into chunks based on size or number of lines. The following bash commands will split the
5.6 GB file, downloaded and unzipped in the previous section, into chunks of 100 MB
each:^['Bash commands'
refer to computer code written in the Bash language. Bash is the default
language used by Linux and Macs for most internal system administration
functions. In Macs, you can open the Bash terminal by typing 'Apple key'-T. In
Windows installing [cygwin](https://www.cygwin.com/) and launching it
will provide access to this functionality. Note: you must start from the correct
*working directory* --- `pwd` in Bash or `setwd()` in R can be used to check this.]

```{r, engine='bash', eval=FALSE}
cd data # change directory
split -b100m npidata_20050523-20150809.csv
```

Assuming there is sufficient
disk space, the output of the above operation should be several 100 Mb text
files: more manageable. These files are named `aa`, `ab` etc.
A sample from the results of this operation can be found in the
`data` folder. This was saved using commands.

```{r, engine='bash', eval=FALSE}
split -l 10 aa mini # further split chunk 'aa' into 10 lines
cp miniaa ../data # copy the first into 'sample-data'
```

\noindent Now the file is much smaller and easy to read: finally we can
read (part of) a 5.6 GB dataset into R using a puny laptop!

```{r}
npi <- read.csv("data/miniaa")
dim(npi) # what are the dimensions of this dataset?
head(npi[c(1, 37)], 3) # view a sample of the data
```

# Filtering with csvkit

[csvkit](https://csvkit.readthedocs.org/en/latest/) is a command-line program
for handling large .csv files, without having to read them all into RAM...

Using the NPI data, the following
[example](https://opendata.stackexchange.com/questions/1256/how-can-i-work-with-a-4gb-csv-file)
illustrates how csvkit can be used to extract useful information from
bulky .csv files before loading the results into R.

# Preprocessing with the LaF package

# Loading static files

Datasets are increasingly becoming continuously 
collected, making them well-suited to databases and other continuous
systems that 'ingest' data in real-time. However, static files are still
probably the most common way to access large datasets and probably will
continue to be so into the future.

This chapter looks at various file-types that are used for storing
large datasets and how R can be used to optimised their read-in.
The most common, simple and in many cases convenient file-type for large datasets
are *plain text* files, so we look at reading these in first, before
exploring more exotic file-types including, `.json`, `.xml`, `.spss`, `.stata`, `.xls`.

# Text files

Data stored as text files are files that are human-readable when displayed
in a 'plain text' editor such as Microsoft Notepad, Vim or R Studio.
Plain text files are the basis of computing.^[Most
programs can be represented as large collections of scripts written in
plain text. R Studio,
for example, is written in 100s of lines of plain text files, all of which
can be viewed on-line
(see [github.com/rstudio/rstudio](https://github.com/rstudio/rstudio)).
This tutorial was written as a UTF-8 encoded plain text '.Rmd' file.
] 
The advantages of plain text files are:

- Simplicity: quick and easy to understand their contents
- Compatibility: text files work with most software packages
- Portability: text files are quick and easy to load, save and share

The disadvantages of plain text files for Big Data are that they can become
unwieldy, even when compressed (remember the 5.6 Gb file from the introduction),
and their ease of modification: text files are certainly not a highly secure
data format.

The most common format of text file is the trusty .csv file, in which
each column is separated by a comma.^[Note that text strings such as
`"speed"` are enclosed in quote marks whereas raw numbers are not.
]


```{r}
write.csv(x = cars[1:3,]) # write a .csv file to the screen
```

```{r, echo=FALSE, eval=FALSE}
write.csv(x = cars[1:3,], "data/minicars.csv") # save to file
```

> **Challenge**: Save a .csv file of the full 'cars' dataset and open it with a plain text editor.

It is important to note that R has its own *binary* data format which minimises the file space occupied by large static datasets.
These can be read and written using the `save()` and `load()` commands, which save the names and contents of multiple R objects into a single file.
We recommend using `saveRDS()` and `readRDS()` instead because they are more flexible, allowing the loaded datasets to be given any name.
To save and re-load the subsetted `cars` dataset, for example, we could use the following code:

```{r}
saveRDS(object = cars[1:3,], file = "data/minicars.Rds")
cars_mini <- readRDS("data/minicars.Rds")
```

Note that the Rdata version of the same data is a third the size of the .csv version:

```{r}
# Report the size of a file from within R using file.size()
file.size("data/minicars.Rds") /
  file.size("data/minicars.csv")
```

Often the benefits of being able to see the data without reading it into R may outweigh the cost of additional hard-disc space.^[The bash command `head data/minicars.csv`, for example, instantly show the top 10 rows of the file, regardless of how large the dataset is, without needing to read it into RAM.
An additional advantage of .csv files over .Rds files is that they display correctly on GitHub.
]

# Freeing your data from spreadsheets

Spreadsheets are ubiquitous in offices around the world, and are used for
storing millions of (mostly quite small) datasets. Nevertheless
Microsoft Excel, the most commonly used spreadsheet program can store
datasets with a maximum size of 1,048,576 rows by 16,384 columns.

There many packages designed for reading spreadsheet files into R, most
of which are of variable reliability. The best of these is
**readxl**, which was found to be much faster
than alternatives from **gdata** and **openxlsx**
packages!^[As
an important aside, this example illustrates the importance of selecting the
*right package*, in addition to the right function and implementation,
for handling large datasets.]

```{r}
f <- "data/CAIT_Country_GHG_Emissions_-_All_Data.xlsx"
system.time(df <- readxl::read_excel(f, sheet = 4))
```

> **Optional challenge:** To brush-up on your benchmarking skills, run tests
to load the same data into R using alternative packages. Which comes closest
to `read_excel()`? Are the results identical?

```{r, echo=FALSE, eval=FALSE}
xls_pkgs <- c("gdata", "openxlsx", "reaODS")
# install.packages(xls_pkgs) # install packages if they're not already
# This took less than 0.1 seconds
system.time(df <- readxl::read_excel(f, sheet = 4))
# This took over 1 minute
system.time(df1 <- gdata::read.xls(f, sheet = 4))
# This took 20 seconds
system.time(df2 <- openxlsx::read.xlsx(f, sheet = 4))

# After saving the spreadsheet to .odt (not included) - took more than 1 minute
system.time(df3 <- readODS::read.ods("data/CAIT_Country_GHG_Emissions_-_All_Data.ods", sheet = 4))

head(df[1:5])
head(df1[1:5])
head(df2[1:5])
head(df3[1:5])
```

To share this dataset with others, it makes sense to save it in a non-proprietary format.
Play with the following commands and see which data format is smallest.

```{r}
write.csv(df, "data/ghg-ems.csv")
saveRDS(df, "data/ghg-ems.Rds")
```

Using `file.size()`, we can ascertain that we've made huge space savings by freeing this dataset from a spreadsheet. The .csv version is `r round(file.size("data/CAIT_Country_GHG_Emissions_-_All_Data.xlsx") / file.size("data/ghg-ems.csv"))` times smaller than the original and the .Rds version is `r round(file.size("data/CAIT_Country_GHG_Emissions_-_All_Data.xlsx") / file.size("data/ghg-ems.Rds"))` times smaller.
These space savings will make a substantial difference to your system resources when dealing with larger datasets gleaned from spreadsheets.

