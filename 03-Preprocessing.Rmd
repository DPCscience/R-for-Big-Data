---
output: pdf_document
---

\chapter{Preprocessing}

R is ideal for handling many tasks but not everything.
As mentioned in the introduction, there are various ways to
preprocess large files outside R to make them easier to handle.
Here we will explore some of the options.

For data stored in large text files we can use
'streaming' utilities before reading it into R. With tools such as
[*sed*](https://www.gnu.org/software/sed/manual/sed.html)
(a 'stream editor' included on most Unix-based systems),
[split](https://en.wikipedia.org/wiki/Split_%28Unix%29) and
[csvkit](https://csvkit.readthedocs.org/en/latest/) a 10 Gb .csv can be
broken up into smaller chunks before being loaded into R.
Here's an 
example of trying (and failing!) to load a large dataset into R.
We recommend you don't
run this code:

```{r, eval=FALSE}
dir.create("data") # create folder for data
url <- "http://download.cms.gov/nppes/NPPES_Data_Dissemination_Aug_2015.zip"

# download a large dataset - don't run
library(downloader) # needs to be installed
download(url, destfile = "data/largefile.zip")
## 550600K .......... ...... 100% 1.26M=6m53s

# unzip the compressed file, measure time
system.time( 
  unzip("data/largefile.zip", exdir = "data/")
  )
##    user  system elapsed 
##  34.380  22.428 193.145

file.info("data/data_20050523-20150809.csv")
##       size: 5647444347
```

The above code illustrates how R can be used to download,
unzip and present information on a giant .csv file, in completely reproducible workflow.
Note that it's 5.6 GB in size and took over 3 minutes to unzip!
The following code requires a 64 bit R installation and will not work on
many laptops.

```{r, eval=FALSE}
df <- read.csv("data/data_20050523-20150809.csv")
## Error: cannot allocate vector of 32.0 Mb
```

We will later explain methods within R for better handling such large datasets
such as using faster read-in functions such as `read_csv()` from the
**readr** package. For now, just remember that reading large datasets into
R can be tricky and time-consuming. Preprocessing outside R can help.
R's interfaces to fast languages can also help.

# Splitting files with Unix tools

The unix utility **split** can be used to split large files up
by size or number of lines. The following bash commands will split the
5.6 GB file, downloaded and unzipped in the first chapter, into chunks of 100 MB
each:^['Bash commands'
refer to computer code written in the Bash language. Bash is the default
language used by Linux and Macs for most internal system administration
functions. In Macs, you can open this by typing 'Apple key'-T. In
Windows, installing [cygwin](https://www.cygwin.com/) and launching it
will provide access to this functionality. Note: you must start from the correct
*working directory*. Use `pwd` in Bash or `setwd()` in R to check this.]

```
cd data
split -b100m npidata_20050523-20150809.csv
```

Assuming there is sufficent
disk space, there output of the above operation should be several 100 Mb text
files which are more manageable. These are named `aa`, `ab` etc.
A sample from the results of this operation can be found in the
`data` folder after the following commands.

```
split -l 10 aa mini # further split chunk 'aa' into 10 lines
cp miniaa ../data # copy the first into 'sample-data'
```

Now the file is much smaller and easy to read: finally we can
read (part of) a 5.6 GB text file into R!

```{r}
npi <- read.csv("data/miniaa")
dim(npi)
head(npi[c(1, 37)], 3)
```

# Filtering with csvkit

[csvkit](https://csvkit.readthedocs.org/en/latest/) is a command-line program
for handling large .csv files, without having to read them all into RAM...

Using the npi data, the following
[example](https://opendata.stackexchange.com/questions/1256/how-can-i-work-with-a-4gb-csv-file)
ilustrates how csvkit can be used to extract useful information from
bulky .csv files before loading the results into R.

# Preprocessing with the LaF package




