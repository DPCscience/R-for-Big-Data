---
output: pdf_document
---

\chapter{Loading static files}

Datasets are increasingly becoming continuously 
collected, making them well-suited to databases and other continuous
systems that 'ingest' data in real-time. However, static files are still
probably the most common way to access large datasets and probably will
continue to be so into the future.

This chapter looks at various file-types that are used for storing
large datasets and how R can be used to optimised their read-in.
The most common, simple and in many cases convenient filetype for large datasets
are *plain text* files, so we look at reading these in first, before
exploring more exotic filetypes including, `.json`, `.xml`, `.spss`, `.stata`, `.xls`.

# Text files

Data stored as text files are files that are human-readable when displayed
in a 'plain text' editor such as Microsoft Notepad, Vim or RStudio.
Plain text files are the basis of computing.^[Most
programs can be represented as large collections of scripts written in
plain text. RStudio,
for example, is written in 100s of lines of plain text files, all of which
can be viewed on-line
(see [github.com/rstudio/rstudio](https://github.com/rstudio/rstudio)).
This tutorial was written as a UTF-8 encoded plain text '.Rmd' file.
] 
The advantages of plain text files are:

- Simplicity: quick and easy to understand their contents
- Compatibility: text files work with most software packages
- Portability: text files are quick and easy to load, save and share

The disadvantages of plain text files for Big Data are that they can become
unwieldy, even when compressed (remember the 5.6 Gb file from the introduction),
and their ease of modification: text files are certainly not a highly secure
data format.

The most common format of text file is the trusty .csv file, in which
each column is separated by a comma.

```{r}
write.csv(x = cars[1:3,]) # write a .csv file to the screen
```

Note that in the result from the above operation text strings such as
`"speed"` are enclosed in quote markes wheras raw numbers are not.

> **Challenge**: Save a .csv file of the full 'cars' dataset and open it with a plain text editor.

# Freeing your data from spreadsheets

Spreadsheets are ubiquitous in offices around the world, and are used for
storing millions of (mostly quite small) datasets. Nevertheless
Microsoft Excel, the most commonly used spreadsheet program can store
datasets with a maximum size of 1,048,576 rows by 16,384 columns,
which is not small data.

The example spreadsheet we'll use can be downloaded with the following
code:

```{r, eval=FALSE}
downloader::download("http://www.ifs.org.uk/bns/bn19figs.xlsx",
  destfile = "data/ineq-ifs.xlsx")
```

There many packages designed for reading spreadsheet files into R, most
of which are of variable reliability. The best of these is
**readxl**, which was found to be more than 100 times faster
than alternatives from **gdata** and **openxlsx**
packages!^[As
an important aside, this example illustrates the importance of selecting the
*right package*, in addition to the right function and implementation,
for handling large datasets.]

```{r}
f <- "data/ineq-ifs.xlsx"
system.time(df <- readxl::read_excel(f, sheet = 15))
```

> **Optional challenge:** To brush-up on your benchmarking skills, run tests
to load the same data into R using alternative packages. Which comes closest
to `read_excel()`? Are the results identical?

```{r, echo=FALSE, eval=FALSE}
xls_pkgs <- c("gdata", "openxlsx")
# This took less than 0.1 seconds
system.time(df <- readxl::read_excel(f, sheet = 15))
# This took over 4 minutes on my machine!
system.time(df1 <- gdata::read.xls(f, sheet = 15))
# This took 30 seconds
system.time(df2 <- openxlsx::read.xlsx(f, sheet = 15))

# After saving the spreadsheet to .odt
system.time(df2 <- openxlsx::read.xlsx(f, sheet = 15))

head(df[1:5])
head(df1[1:5])
head(df2[1:5])
identical(df, df1)
identical(df, df2)
```



