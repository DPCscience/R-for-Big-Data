---
output: pdf_document
---

\chapter{Manipulating Big Data}

```{r, echo=FALSE}
library("xtable")
library("tidyr")
library("readr")
```


# Reshaping data

A key skill in data analysis is understanding the 'shape' of datasets and being able to 'reshape' them.
An example of the various shapes that the same datasets can assume is provided by @tidy-data and illustrated in Tables \ref{Tpew} and \ref{Tpewt}.

```{r, echo=FALSE, eval=FALSE}
# Download data from its original source - an academic paper
downloader::download("http://www.jstatsoft.org/v59/i10/supp/4", destfile = "v59i10-data.zip")
# The source code associated with the paper
downloader::download("http://www.jstatsoft.org/v59/i10/supp/3", destfile = "data/reshape/v59i10.R")
# After running the R script...
dir.create("data/reshape")
unzip("v59i10-data.zip", exdir = "data/reshape/")
# write.csv(raw, "data/reshape-pew.csv")
```

```{r, echo=FALSE, eval=FALSE}
raw <- read_csv("data/reshape-pew.csv")
raw <- raw[-c(1,ncol(raw))] # remove excess cols
names(raw) <- c("religion", "<$10k", "$10--20k", "$20--30k", "$30--40k", "$40--50k", 
"$50--75k", "$75--100k", "$100--150k", ">150k"
)
print.xtable(xtable(raw[1:3,1:4], caption = "First 6 rows of the aggregated 'pew' dataset from Wickham (2014a) in an 'untidy' form.", include.rownames = F), comment = FALSE, include.rownames = F)
rawt <- gather(raw, Income, Count, -religion)
head(rawt)
tail(rawt)
rawt$Count <- as.character(rawt$Count)
rawt$Income <- as.character(rawt$Income)
rawtp <- rawt[c(1:3, nrow(rawt)),]

insertRow <- function(existingDF, newrow, r) {
  existingDF[seq(r+1,nrow(existingDF)+1),] <- existingDF[seq(r,nrow(existingDF)),]
  existingDF[r,] <- newrow
  existingDF
}

rawtp <- insertRow(existingDF = rawtp, newrow = rep("...", 3), r = 4)
xtable(rawtp)
```

\begin{margintable}
\centering
\begin{tabular}{lrrr}
  \toprule
religion & $<$\$10k & \$10--20k & \$20--30k \\ 
  \midrule
Agnostic &  27 &  34 &  60 \\ 
  Atheist &  12 &  27 &  37 \\ 
  Buddhist &  27 &  21 &  30 \\ 
\bottomrule
\end{tabular}
\vspace{0.2cm}
\caption{First 3 rows and 4 columns of the aggregated 'Pew' dataset from Wickham (2014a) in an 'untidy' form.}\label{Tpew}
\vspace{2cm}
\end{margintable}


\begin{margintable}
\centering
\begin{tabular}{rrl}
  \toprule
 religion & Income & Count \\ 
  \midrule
Agnostic & $<$\$10k & 27 \\ 
Atheist & $<$\$10k & 12 \\ 
Buddhist  & $<$\$10k & 27 \\ 
... & ... & ... \\ 
Unaffiliated  & $>$150k & 258 \\ 
\bottomrule
\end{tabular}
\vspace{0.2cm}
\caption{First 3 and last rows of the 'tidied' Pew dataset.}\label{Tpewt}
\end{margintable}

These tables may look very different, but they contain precisely the same data.
They have been reshaped, such that column names in the 'flat' form in Table \ref{Tpew} became a new variable in the 'long' form in Table \ref{Tpewt}.
According to the concept of 'tidy data' [@tidy-data], the long form is correct.
Note that 'correct' here is used in the context of data analysis and graphical visualisation.
For tabular presentation (i.e. tables) the 'wide' or 'untidy' form may be better.

Tidy data has the following characteristics [quoting @tidy-data]:

1. Each variable forms a column.
2. Each observation forms a row.
3. Each type of observational unit forms a table.

Because there is only one observational unit in the example (religions), it can be described in a single table.
Large and complex datasets are usually represented by multiple tables, with unique identiers or 'keys' to join them together [@Codd1979].
Being able to manipulate your data into a tidy and relational form is important for Big Data work because this form minimises data duplication and facilitates fast
code.^[Because
R is a vectorised language, it adept at handling long 1 dimensional vectors but less so at handling many interrelated
columns.]
Due to the importance of tidying data, an entire package, aptly named **tidyr** has been developed for the purpose.
The process of tidying datasets usually includes one or many of the following processes, each of which has an associated **tidyr** function:

- Re-arranging 'wide' tables to become 'long' so that column names become a new variable. . This is illustrated in Tables \ref{Tpew} to \ref{Tpewt} and can be acheived with the function `gather`.


# Filtering columns

Often Big Data contains much worthless or blank information.
An example of this is provided in the 


generally, that becomes especially important when handling large datasets, is 

# Data aggregation

# dplyr

**dplyr** has been designed to make data analysis
fast and intuitive [@dplyr]. **dplyr** works perfectly
on `data.frames` but its default object is the `tbl`, which
is identical to a `data.frame` but prints
objects differently (more usefully), as shown below.

```{r, message=FALSE}
idata <- readxl::read_excel("data//world-bank-ineq.xlsx", sheet = 1)
library(dplyr)
idata <- tbl_df(idata) # convert the dataset to tbl class
idata # print the dataset in the dplyr way
```

**dplyr** is much faster than base implementations of various
operations, but it has the potential to be even faster, as
*parallelisation* is
[planned](https://github.com/hadley/dplyr/issues/145).

You should not be expecting to learn the **dplyr** package in one sitting:
the package is large and can be seen as
an entirely new language, to supplement R's,
in its own right. Following the 'walk before you run' principle,
we'll start simple, by replicating the subsetting
and grouping operations undertaken in base R above.

```{r}
aus_idata2 <- filter(idata, Country == "Australia")
```

Note that we did not need to use the `$` to tell R
that `Country` is a variable of the `idata` object.
Because `idata` was the first argument, **dplyr** 'knew'
that any subsequent names would be variables.

> **Note**: this *syntax* is a defining feature of **dplyr**
and many of its functions work in the same way.

The **dplyr** equivalent of aggregate is to use
the grouping function `group_by` in combination with
the general purpose function `summarise` (not to
be confused with `summary` in base R).

```{r}
names(idata)[5:9] <-
  c("top10", "bot10", "gini", "b40_cons", "gdp_percap")
```

As we will see with Twitter data in a subsequent section,
the *class* of R objects is critical to how it performs.
If a class is incorrectly specified (if numbers are treated
as factors, for example), R will likely generate error messages.
Try typing `mean(idata$gini)`, for example.

We can re-assign the classes of the numeric variables
one-by one:

```{r}
idata$gini <- as.numeric(as.character(idata$gini))
mean(idata$gini, na.rm = TRUE) # now the mean is calculated
```

However, the purpose of programming languages is to *automate*
arduous tasks and reduce typing. The following command
re-classifies all of the numeric variables using
the `apply` function (we'll seem more of `apply`'s relatives
later):

```{r, warning=FALSE}
idata[5:9] <- apply(idata[5:9], 2,
  function(x) as.numeric(as.character(x)))
```

```{r}
countries <- group_by(idata, Country)
summarise(countries, gini = mean(gini, na.rm = T))
```

Note that `summarise` is highly versatile, and can
be used to return a customised range of summary statistics:

```{r tidy=FALSE}
summarise(countries,
  # number of rows per country
  obs = n(), 
  med_t10 = median(top10, na.rm = T),
  # standard deviation
  sdev = sd(gini, na.rm = T), 
  # number with gini > 30
  n30 = sum(gini > 30, na.rm = T), 
  sdn30 = sd(gini[ gini > 30 ], na.rm = T),
  # range
  dif = max(gini, na.rm = T) - min(gini, na.rm = T)
  )
```

To showcase the power of `summarise` used on
a `grouped_df`, the
above code reports a wide range of customised
summary statistics
*per country*: 

- the number of rows in each country group
- standard deviation of gini indices
- median proportion of income earned by the top 10%
- the number of years in which the gini index was greater than 30
- the standard deviation of gini index values over 30
- the range of gini index values reported for each country.

> **Challenge**: explore the **dplyr**'s documentation, starting with the introductory vignette, accessed by entering `vignette("introduction")` and test out its capabilities on the `idata` dataset. (More vignette names can be discovered by typing `vignette(package = "dplyr")`)

# Chaining operations with dplyr

Another interesting feature of **dplyr** is its ability
to chain operations together. This overcomes one of the
aesthetic issues with R code: you can end end-up with
very long commands with many functions nested inside each
other to answer relatively simple questions.

> What were, on average, the 5 most unequal
years for countries containing the letter g?

Here's how chains work to organise the analysis in a
logical step-by-step manner:

```{r}
idata %>% 
  filter(grepl("g", Country)) %>%
  group_by(Year) %>%
  summarise(gini = mean(gini, na.rm = T)) %>%
  arrange(desc(gini)) %>%
  top_n(n = 5)
```

The above function consists of 6 stages, each of which
corresponds to a new line and **dplyr** function:

1. Filter-out the countries we're interested in (any selection criteria could be used in place of `grepl("g", Country)`).
2. Group the output by year.
3. Summarise, for each year, the mean gini index.
4. Arrange the results by average gini index
5. Select only the top 5 most unequal years.

To see why this method is preferable to the nested
function approach, take a look at the latter.
Even after indenting properly it looks terrible
and is almost impossible to understand!

```{r}
top_n(
  arrange(
    summarise(
      group_by(
        filter(idata, grepl("g", Country)),
        Year),
      gini = mean(gini, na.rm = T)),
    desc(gini)),
  n = 5)
```

Of course, you *could* write code in base R to
undertake the above analysis but for many
people the **dplyr** approach is the most agreable to write.

