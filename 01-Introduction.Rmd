---
output: pdf_document
---

```{r echo=FALSE}
library("pryr")
library("grid")
library("png")
```

\chapter{Big Data and R}

# What is Big Data?

A common definition of Big Data is data that
is:^[Data here is treated as a mass noun, similar to information.
Purists may insist that 'data' should always plural because it originates from the purl of Latin word datum, but language evolves and the singular form of the word is becoming more and more common [@kitchin2014data].]

- Variable, within each dataset and between sources
- Voluminous, occupying much RAM and hard disk space
- High in velocity: it's always being generated

Precisely how variable, voluminous and rapidly generated data needs to be before it's classified as 'Big' is rarely specified, however.
Looser definitions recognise that Big Data is an umbrella or 'catch all' term: in practice Big Data is often used to refer to information that is simply tricky to analyse using established methods [@Lovelace2015].

The variety of new datasets becoming available
is such that no single tutorial can
cover all shapes and sizes.
A solid understanding of the *language* used to interact with data,
in this case R, will provide the flexibility to deal with this diversity.
Understanding computational bottlenecks and writing computationally efficient code will help deal with  data volume.
This means that becoming proficient in handling Big Data entails first becoming fluent in data analysis and computing more generally.

It's also easy to get side-tracked when analysing large datasets so clearly defining the aim of a particular analysis project is usually more important than knowing the 'right' function or package.
There are often many ways to solve a problem with R. In addition to computational speed, the 'best' option will likely depend on additional considerations:

- ease and speed of writing the code
- ease of communicating and reproducing the analysis
- durability of code

```{r drill, fig.margin=TRUE, fig.cap= "A drill is analogous to a software tool: the questions of functionality and reliability should trump the question: 'is it the best?'", echo=FALSE}
grid.raster(readPNG("figures//746px-Pistol-grip_drill.svg.png"))
```

In this context it is useful to think of software as a power-tool (Fig. 1.1). People rarely ask 'is this the BEST possible drill'?.
More likely a good builder will ask: 'Is this drill *good enough* to get the job done?' 'Is this drill robust?' and 'Will it work in 20 years time?' The same applies to R for Big Data.

Regardless of the 'big' dataset you hope to use,
you can be confident of one thing:
**it is unlikely to be ready to analyse.**
This means that you must work to tidy the data,
a task that typically takes around
80% of the effort expended on data analysis projects
[@tidy-data]. 

# Coping with Big Data in R

R has had a difficult relationship with big data. One of R's default key features is that it loads data into the computer's
RAM.^[There are, however, packages such as **dplyr** which allow R to access, filter and even process data stored remotely.
These are described in chapter 5.]
This was less of a problem twenty years ago, when data sets were small and the main bottleneck on analysis was how quickly a statistician could think. Traditionally, the development of a statistical model took more time than the computation. When it comes to Big Data, this changes.
Nowadays datasets that are larger than your laptop's memory are commonplace.

> **Challenge**: How much RAM do you have? Hint: Google if necessary.

Even if the original data set is relatively small data set, the analysis can generate large objects. For example, suppose we went to perform standard cluster analysis. Using the built-in data set `USAarrests`, we can calculate a distance matrix,
```{r}
d = dist(USArrests, method = "euclidean")
```

and perform hierarchical clustering to get a dendrogram
```{r}
fit = hclust(d)
```

to get a dendrogram
```{r fig.fullwidth=TRUE, fig.height=2, echo=2, fig.cap="Dendrogram from USArrests data."}
par(mar=c(3,3,2,1), mgp=c(2,0.4,0), tck=-.01,cex=0.5, las=1)
plot(fit, labels=rownames(d))
```

When we inspect the object size of the original data set and the distance object
```{r}
object_size(USArrests)
object_size(d)
```

we have managed to create an object that is three times larger than the original data set. In fact the object `d` is a symmetric $n \times n$ matrix, where $n$ is the number of rows in `USAarrests`. Clearly, as `n` increases the size of `d` increases at rate $O(n^2)$. So if our original data set contained $10,000$ records, the associated distance matrix would contain almost $10^8$ values. Of course, since the matrix is symmetric, this corresponds to almost $50$ million unique values.

To tackle big data in R, we review some of the possible strategies available.

## Buy more RAM

Since R keeps all objects in memory, the easiest way to deal with memory issues. Currently, 16GB costs less than £100. This small cost is quickly recuperated on user time. A relatively powerful desktop machine can be purchased for less that £1000. 

## Sampling

Do you **really** need to load all of data at once? For example, if your data contains information regarding sales, does it make sense to aggregate across countries, or should the data be split up? Assuming that you need to analyse all of your data, then random sampling could provide an easy way to perform your analysis. In fact, it is almost always sensible to sample your data set at the beginning of an analysis until your analysis pipeline is in reasonable shape.

If your dataset is too large to read into RAM, it may need to be
*preprocessed* or *filtered* using tools external to R before
reading it in. This is the topic of chapter 3.
For databases we can filter when asking for the data from within R
(described in chapter 6).

## Integration with C++ or Java

Another strategy to improve performance, is to move small parts of the program from R to another, faster language, such as C++ or Java. The goal is to keep R's neat way of handling data, with the higher performance offered by other languages. Indeed, many of R's base functions are written in C or Fortran. This outsourcing of code to another language can be easily hidden in another function. 

## Avoid storing objects in memory

There are packages available that avoid storing data in memory. Instead, objects are stored on your hard disc and analysed in blocks or chunks. Hadoop is an example of this technique. This strategy is perfect for dealing with large amounts of data. Unfortunately, many algorithms haven't been designed with this principle in find. This means that only a few R functions that have been explicitly created to deal with specific chunk data types will work.

The two most famous packages on CRAN that use this principle are `ff` and ``ffbase`. The commercial product,  Revolution R Enterprise, also uses the chunk strategy in their `scaleR` package.

## Alternative interpreters

Due to the popularity of R, it now possible to use alternative interpreters (the interpreter is where the code is run). There are currently four possibilities

 * [pqrR](http://www.pqr-project.org/) (pretty quick R) is a new version of the R interpreter. One major downside, is that it is based on R-2.15.0. The developer (Radford Neal) has made many improvements, some of which have now been incorporated into base R. **pqR** is an open-source project licensed under the GPL. One notable improvement in pqR is that it is able to do some numeric computations in parallel with each other, and with other operations of the interpreter, on systems with multiple processors or processor cores.
 
  * [Renjin](http://www.renjin.org/) reimplements the R interpreter in Java, so it can run on the Java Virtual Machine (JVM). Since R will be pure Java, it can run anywhere.

  * [Tibco](http://spotfire.tibco.com/) created a C++ based interpreter called TERR. 

  * Oracle also offer an R-interpreter, that uses  Intel's mathematics library and therefore achieves a higher performance without changing R's core. 


