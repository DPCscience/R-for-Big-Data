---
output: pdf_document
---

\chapter{Memory Matters}

Whenever big data comes up, the obvious question to ask, is, what do you mean by big? The answer to this question is that everything is relative. For example, a big data set from thirty years ago, could probably be processed with ease using todays computers. Indeed, a data set we consider "big", Google/Facebook would consider small.

When talking about Big Data, we need a comparison. In this course, **big** is relative to our available resources.

# File sizes

A computer cannot store "numbers" or "letters". The only thing a computer can store and work with is bits. A bit is binary, it is either a $0$ or a $1$\sidenote{Or `TRUE`/`FALSE` or `yes`/`no`.}. In fact from the computers point of view, a bit is just a blip of electricity that either is or isn't there.

In the past we used the ASCII character set. This set defined $128$ characters including $0$ to $9$, upper and lower case alpha-numeric and a few control characters such as a new line. To store these characters required $8$ bits\sidenote{Actually, it only required $7$ bits since $2^7 = 128$, but $8$ bits were typically used: see \url{http://stackoverflow.com/q/14690159/203420}}. Table \ref{T2.1} gives the binary representation of the first few characters.

\begin{margintable}
\centering
\begin{tabular}{@{}ll@{}}
\toprule
Bit representation & Character\\
\midrule
$01000001$ & A\\
$01000010$ & B\\
$01000011$ & C\\
$01000100$ & D\\
$01000101$ & E\\
$01010010$ & R \\
\bottomrule
\end{tabular}
\vspace{0.2cm}
\caption{The bit representation of a few ASCII characters.}\label{T2.1}
\end{margintable}

It's worth pointing out that storing characters has got more sophisticated in recent years. In particular, Unicode was created to try to create a single character set that included every reasonable writing system. See 
\begin{center}
\url{http://www.joelonsoftware.com/articles/Unicode.html}
\end{center}
\noindent for a nice introduction on character encoding.

Eight bits is one byte\sidenote{Four bits is called a nibble.}. So two characters would use two bytes or 16 bits. A document containing $100$ characters would use $100$ bytes ($800$ bits). This assumes that the file didn't have any other memory overhead, such as font information or meta-data\sidenote{An empty `.docx` file takes about 3.7KB of storage.}.

When people first started to think about computer memory, they noticed that $2^{10} \simeq 1000$ and $2^{20} \simeq 10^6$, so they adopted the short hand of kilo- and mega-bytes. Of course, \textit{everyone} new that it was just a short hand, and it was really a binary power. When computers became more wide spread, foolish people like you and me just assumed the kilo did actually mean $10^3$ bytes.  Fortunately, the IEEE Standards Board decided that IEEE standards will use the conventional, internationally adopted, definitions of the SI prefixes. So a kilobyte (KB) is 1000 bytes and a megabyte (MB) is 1000 kilobytes (see table \ref{T2.2}). A petabyte is approximately 100 million drawers filled with text. Astonishingly, Google processes around $20$ petabytes of data every day. In 28th August, 2015 over 1 billion\sidenote{I presume this is an American Billion - so one Giga user, or Guser?} users logged on to facebook.

\begin{table}[t]
\centering
\begin{tabular}{@{}llll@{}ll @{}}
\toprule
Factor & 	Name &	Symbol &	Origin& & Derivation \\
\midrule
$2^{10}$& kibi &	Ki&	Kilobinary: &$(2^{10})^1$ 	& Kilo: $(10^3)^1$ \\
$2^{20}$&	mebi &	Mi&	Megabinary: &$(2^{10})^2$ 	& Mega: $(10^3)^2$ \\
$2^{30}$&	gibi &	Gi&	Gigabinary: &$(2^{10})^3$	 & Giga: $(10^3)^3$ \\
$2^{40}$&	tebi &	Ti&	Terabinary: &$(2^{10})^4$	 & Tera: $(10^3)^4$ \\
$2^{50}$&	pebi &	Pi&	Petabinary: &$(2^{10})^5$	 & Peta: $(10^3)^5$ \\
\bottomrule
\end{tabular}
\vspace{0.2cm}
\caption{Data conversion table \textbf{XXX} \url{http://physics.nist.gov/cuu/Units/binary.html}}\label{T2.2}
\end{table}

Different data types, such as (ASCII) characters, integers and doubles, have different storage costs (see table \ref{T2.3}). In R, we don't tend to worry about characters, integers, or doubles. We just let R do the thinking and conversion. However it is helpful to understand why files are large and the limits of the computer system.

\begin{margintable}
\centering
\begin{tabular}{@{} ll @{}}
\toprule
Type & Amount (Bytes)\\
\midrule
Character & 1 \\
Integer & 4 \\
Double & 8 \\
\bottomrule
\end{tabular}
\vspace{0.2cm}
\caption{Storage space for standard data types}\label{T2.3}
\end{margintable}

\subsection*{Exercises}

\begin{enumerate}
\item R loads everything into memory, i.e. your computers RAM. How much RAM do you have\sidenote{Feel free to Google, "How much RAM do I have?"}?
\item Suppose you had a file with a single column of doubles. Approximately, what is the maximum number of rows you could load into R?
\item Using Google, how much does it cost (in pounds) to double the amount of available RAM?
\item (Hard) What is the range of values an integer object can represent?
\end{enumerate}


# Object size in R

When thinking about sizes of objects in R, it's a little bit more complicated than simply multiplying the data type by the number of bytes - for full description see @Wickham2014. For example when we create an R vector, memory is allocated for a number of things.

 * Object meta data: this is information on the base data type and memory management. For example, is the object a logical, character, numeric or complex data type?
 * Three pointers: these are addresses to where memory is stored on the hard drive. One of the pointers is used to access the attribute list. The other two pointers help R move between object on your hard drive.
 
\noindent Since asking for more memory is a relatively expensive operation, R asks for more than is needed when growing objects. In particular, R's vectors are always $2^3=8$, $2^4=16$, $2^5=32$, $2^6=64$ or $2^7=128$ bytes long. After $128$ bytes, R only asks memory in multiples of $8$ bytes.

We can examine the size of an object using the base function `object.size`. However, `object_size` in the `pryr` package is a similar function that counts more accurately and includes the environment size. Let's start with a simple vector\marginnote{For most operations we don't worry about integers, but for frequently used functions, there is a small saving. To create an integer explicit in R, we use `L`, as in $4L$. However, the largest possible integer that can be created is $2^{31}-1$. Try \texttt{as.integer(2$\wedge$31)}.}
```{r}
v1 = 1:1e6
```
\noindent When we use the `:` operator, we are actually creating a vector of integers. Remember that an integer is only $4$ bytes, so this is more efficient. To manually calculate the object size of `v1`, we have
\[
4\times 10^6 \,\text{bytes} \simeq  4 \,\text{MB}
\]
This corresponds with 
```{r}
library("pryr")
object_size(v1)
```
\noindent If we create a similar vector using the sequence command
```{r}
v2 = seq(1, 1e6, by=1)
object_size(v2)
```
\noindent we find that the size of `v2` is double that of `v1`. This is because when we use the `:` operator we create a vector with type `integer`, whereas the `seq` command has created a vector of `doubles` (see table \ref{T2.3}).

R is also tries to avoid making unnecessary copies of objects. For example, consider the following two lists
```{r}
l1 = list(v1, v1)
l2 = list(v1, v2)
```
\noindent When we investigate the object sizes, we see that `v1` hasn't been double counted in `l1`\marginnote{This clever use of optimising memory is relatively new to R. This is why it is always worthwhile upgrading R to the latest version. Small optimisations to a data frame, can make a big difference to your code.}
```{r}
object_size(l1)
object_size(l2)
```
\noindent Moreover, if we look at the combined size of the two lists, 
```{r}
object_size(l1, l2)
object_size(l1, l2)
```
\noindent we still see that `v1` has only been counted once.

\subsection*{Exercise}

Create some standard R objects, such as lists, data frames and matrices. Compare their sizes.

# Integers, doubles and other data types

When programming in C or FORTRAN, we have to specify the data type of every object we create. The benefit of this is that the compiler can perform clever optimisation\sidenote{If you have a spare evening, it's interesting to look at the compiler optimisation page on wikipedia: \url{https://en.wikipedia.org/wiki/Optimizing_compiler}. Some of the optimisations are amazing}. The downside is that programme length is longer. In R we don't tend to worry about about data types. For the most part, numbers are stored in double-precision floating-point format. But R does have other ways of storing numbers.\marginnote[1cm]{See \url{https://goo.gl/ZA5R8a} for a description on floating point numbers.}
  
  * `numeric`: the `numeric` function is the same as a `double`. However, `is.numeric` is also true for integers.
  * `single`: R doesn't have a single precision data type. Instead, all real numbers are stored in double precision format. The functions `as.single` and `single` are identical to `as.double` and `double` except they set the attribute `Csingle` that is used in the `.C` and `.Fortran` interface.
  * `integer`: Integers exist to be passed to C or Fortran code. Typically, we don't worry about creating integers. However, they are occasionally used to optimise subsetting operations. When we subset a data frame or matrix, we are interacting with C code. If we look at the arguments for the `head` function
    ```{r}
    args(head.matrix)
    ```
    The default argument is `6L` (the `L` is creating an integer object). Since this function is being called by almost everyone that uses R, this low level optimisation is useful.
 
# Collecting the garbage

The `object_size` function tells you the size of a particular object. The function `mem_used`\marginnote{The function `mem\_used` is also from the `pryr` package.} tells you the amount of memory that is being using by `R`. Since managing memory is a complex process, determining the exact amount of memory used isn't exact; it isn't obvious what we mean by *memory used*. The value returned by `mem_used` only includes objects created by R, not R itself. Also manipulating memory is an expensive operation, so the OS and R are lazy at reclaiming memory (this is a good thing).

In some languages, such as C, the programmer has the fun task of being in charge of managing memory. Every time the programmer asks for more memory using `malloc`\sidenote{This is the function used in C.} there should be a corresponding call (somewhere) to `free`. When the call to `free` is omitted, this is known as a memory leak. In R we don't have to worry about freeing memory; the garbage collector takes care of it. For example, consider the following example. We define a function `g`.\marginnote{Since we have used `mem\_used` inside a message statement, the automatic rounding disappears, since we are no longer calling `print.Bytes`.} 

```{r}
g = function() {
  z = 1:1e7
  message("Mem used: ", 
          round(mem_used()/10^6), "MB")
  0
}
```

\noindent and calculate the current memory being used

```{r}
mem_used()
```

\noindent When we call `g` and calculate the memory used after the call

```{r}
x = g()
mem_used()
```

\noindent The memory usage hasn't changed. Since the variable `z` is only referenced inside the function, the associated memory is freed after the function call has ended.

We can force a call to the garbage collector, via `gc()` or by explicitly deleting the object with `rm`. However, this is almost never needed. R is perfectly able to manage it's own memory and you need to use `gc()` or `rm()` to clean up.

# Monitoring memory change

There are tools available to dynamically monitor changes in memory. The first, `pryr::mem_change`, is useful for determining the effect of an individual command. R also comes with a memory profiler, `utils::Rprof`. However, the resulting outputting is tricky to understand. The lineprof\sidenote{\url{https://github.com/hadley/lineprof}} package is more user friendly. However, it is currently only available from github and requires a development environment, i.e. a C compiler. The development environment is only an issue for Windows users.

# HDD vs SSD

Unless you have a fairly expensive laptop, you've probably got a standard hard disk drive (HDD). HDDs were first introduced by IBM in 1956. Data is stored using magnetism on a rotating platter. The faster the platter spins, the faster the HDD can perform. Many laptop drives spin at either 5400RPM (Revolutions per Minute) or 7200RPM. The major advantage of HDD is that they are cheap; a 1TB laptop is becoming standard.

\begin{marginfigure}
\centering
\includegraphics[]{figures/Laptop-hard-drive-exposed.jpg}
\caption{A standard 2.5" hard drive, found in most laptops. Figure credit: \url{https://en.wikipedia.org/wiki/Hard_disk_drive}.}
\end{marginfigure}

Solid state drives (SSD), can be thought of large, but more sophisticated versions of your USB stick. They have no moving parts and information is stored in microchips. Since there are no moving parts, reading/writing is much quicker\sidenote{SSDs have other benefits, such as being quieter, faster booting time for the OS, required less power (more battery life).}.

The read/write speed for a standard HDD is usually in the region of 50-120MB/s (usually nearer the 50 than the 120). For SSDs, speeds are typically over 200MB/s and for top of the range models, it is closer to 500MB/s. If you're wondering, read/write speeds for RAM is around 2-20GB/s. So at best, SSDs are at least one order of magnitude slower than RAM, but still faster than standard HDDs.

































